# -*- coding: utf-8 -*-
"""Saliency.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kBmHDRkrdbvxKtmtcoOrIBjlnK82nX0e
"""

from __future__ import print_function
import re
from time import perf_counter
from numpy.core.fromnumeric import mean, transpose
from numpy.lib.function_base import select
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.modules.upsampling import Upsample
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import os
import imageio
import numpy as np
import torchvision
from torchvision import transforms
from tqdm import tqdm
import matplotlib.pyplot as plt
import pandas as pd
from PIL import Image

#pip install torchsummary

#from torchsummary import summary

#from google.colab import drive
#drive.mount('/content/drive')



class Encoder(nn.Module):
  def __init__(self, model_dict=None):
    super().__init__()


    self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
    self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)

    self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
    self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)

    self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
    self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
    self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)

    self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)
    self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
    self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)

    self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
    self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
    self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
   

    if model_dict is not None:

        self.conv1_1.weight.data = model_dict['conv1_1_weight']
        self.conv1_1.bias.data = model_dict['conv1_1_bias']

        self.conv1_2.weight.data = model_dict['conv1_2_weight']
        self.conv1_2.bias.data = model_dict['conv1_2_bias']

        self.conv2_1.weight.data = model_dict['conv2_1_weight']
        self.conv2_1.bias.data = model_dict['conv2_1_bias']

        self.conv2_2.weight.data = model_dict['conv2_2_weight']
        self.conv2_2.bias.data = model_dict['conv2_2_bias']

        self.conv3_1.weight.data = model_dict['conv3_1_weight']
        self.conv3_1.bias.data = model_dict['conv3_1_bias']

        self.conv3_2.weight.data = model_dict['conv3_2_weight']
        self.conv3_2.bias.data = model_dict['conv3_2_bias']

        self.conv3_3.weight.data = model_dict['conv3_3_weight']
        self.conv3_3.bias.data = model_dict['conv3_3_bias']

        self.conv4_1.weight.data = model_dict['conv4_1_weight']
        self.conv4_1.bias.data = model_dict['conv4_1_bias']

        self.conv4_2.weight.data = model_dict['conv4_2_weight']
        self.conv4_2.bias.data = model_dict['conv4_2_bias']

        self.conv4_3.weight.data = model_dict['conv4_3_weight']
        self.conv4_3.bias.data = model_dict['conv4_3_bias']

        self.conv5_1.weight.data = model_dict['conv5_1_weight']
        self.conv5_1.bias.data = model_dict['conv5_1_bias']

        self.conv5_2.weight.data = model_dict['conv5_2_weight']
        self.conv5_2.bias.data = model_dict['conv5_2_bias']

        self.conv5_3.weight.data = model_dict['conv5_3_weight']
        self.conv5_3.bias.data = model_dict['conv5_3_bias']

  def forward(self, xb):
    xb = F.relu(self.conv1_1(xb))
    xb = F.relu(self.conv1_2(xb))
    xb = F.max_pool2d(xb, 2)
    xb = F.relu(self.conv2_1(xb))
    xb = F.relu(self.conv2_2(xb))
    xb = F.max_pool2d(xb, 2)
    xb = F.relu(self.conv3_1(xb))
    xb = F.relu(self.conv3_2(xb))
    xb = F.relu(self.conv3_3(xb))
    xb = F.max_pool2d(xb, 2)
    xb = F.relu(self.conv4_1(xb))
    xb = F.relu(self.conv4_2(xb))
    xb = F.relu(self.conv4_3(xb))
    xb = F.max_pool2d(xb, 2)
    xb = F.relu(self.conv5_1(xb))
    xb = F.relu(self.conv5_2(xb))
    xb = F.relu(self.conv5_3(xb))

    return xb

class Decoder(nn.Module):
  def __init__(self):
    super().__init__()
    
 
    self.conv6_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
    self.conv6_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
    self.conv6_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
 
    self.conv7_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
    self.conv7_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
    self.conv7_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
 
    self.conv8_1 = nn.Conv2d(512, 256, kernel_size=3, padding=1)
    self.conv8_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
    self.conv8_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
 
    self.conv9_1 = nn.Conv2d(256, 128, kernel_size=3, padding=1)
    self.conv9_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
 
    self.conv10_1 = nn.Conv2d(128, 64, kernel_size=3, padding=1)
    self.conv10_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
    self.conv10_3 = nn.Conv2d(64, 1, kernel_size=1, padding=0)
 
 
  def forward(self, xb):
    xb = F.relu(self.conv6_1(xb))
    xb = F.relu(self.conv6_2(xb))
    xb = F.relu(self.conv6_3(xb))
    upsample = nn.Upsample(scale_factor= 2, mode= 'bilinear', align_corners= False)
    xb = upsample(xb)
    xb = F.relu(self.conv7_1(xb))
    xb = F.relu(self.conv7_2(xb))
    xb = F.relu(self.conv7_3(xb))
    upsample = nn.Upsample(scale_factor= 2, mode= 'bilinear', align_corners= False)
    xb = upsample(xb)
    xb = F.relu(self.conv8_1(xb))
    xb = F.relu(self.conv8_2(xb))
    xb = F.relu(self.conv8_3(xb))
    upsample = nn.Upsample(scale_factor= 2, mode= 'bilinear', align_corners= False)
    xb = upsample(xb)
    xb = F.relu(self.conv9_1(xb))
    xb = F.relu(self.conv9_2(xb))
    upsample = nn.Upsample(scale_factor= 2, mode= 'bilinear', align_corners= False)
    xb = upsample(xb)
    xb = F.relu(self.conv10_1(xb))
    xb = F.relu(self.conv10_2(xb))
    xb = self.conv10_3(xb)
 
    return xb

class Generator(nn.Module):
    def __init__(self, model_dict=None):
        super().__init__()
        self.encoder = Encoder(model_dict)
 
        for param in self.encoder.parameters():
            param.requires_grad = False
        # do training only for decoder
        self.decoder = Decoder()

    def forward(self, xb):
        xb = self.encoder(xb)
        xb = self.decoder(xb)
        #apply sigmoid activation only while getting predictions of fixationny. Not while training
        if not self.training:
            return torch.sigmoid(xb)
        else: 
            return xb
        #return xb

class BCELossWithDownsampling():
    def __init__(self):
      self.downsample = torch.nn.AvgPool2d(kernel_size=4, stride=4, count_include_pad=False)
      self.loss_fcn = torch.nn.BCEWithLogitsLoss()

    def __call__(self, pred, y):
      #print('downsampled pred shape',self.downsample(pred).shape)
      #print('downsampled target shape',self.downsample(y).shape)
      return self.loss_fcn(self.downsample(pred), self.downsample(y))

class MSELoss():
    def __init__(self) -> None:
      self.loss_fcn = torch.nn.functional.mse_loss()
    def __call_(self, pred, y):
      return self.loss_fcn(pred, y)

class FixationDataset(Dataset):
    def __init__(self, root_dir, image_file, fixation_file, transform):
        self.root_dir = root_dir
        self.image_files = read_file(os.path.join(self.root_dir, image_file))
        if not fixation_file:
            print("No fixation files provided")
            self.fixation_files = []
            
        else:
            self.fixation_files = read_file(
                os.path.join(self.root_dir, fixation_file))
            assert(len(self.image_files) == len(self.fixation_files))
        

        self.transform = transform

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        img_name = os.path.join(self.root_dir, self.image_files[idx])
        image = imageio.imread(img_name)
        

        if self.fixation_files:
            fix_name = os.path.join(self.root_dir, self.fixation_files[idx])
            fix = imageio.imread(fix_name)
            #print('fix file shape', fix.shape)
        else:
            fix = np.zeros((224, 224)) #if there are no fixation files return np array with zeroes

        sample = {'image': image, 'fixation': fix, 'image_filename': os.path.basename(img_name)}

        if self.transform:
            sample = self.transform(sample)

        return sample

class Rescale():
    def __init__(self):
        pass
    def __call__(self, sample):
        imageRescaled = sample['image']
        fixationRescaled =  sample['fixation']
        imageRescaled = imageRescaled.astype(np.float32)
        fixationRescaled = fixationRescaled.astype(np.float32)
        imageRescaled = imageRescaled / 255.0
        fixationRescaled = fixationRescaled / 255.0

        return {'image': imageRescaled, 'fixation': fixationRescaled, 'image_filename':sample['image_filename']}


class ToTensor():
	def __init__(self):
		pass
	
	def __call__(self, sample):
		imageTransposed = np.transpose(sample['image'], (2, 0, 1)) 
		imageTransposed = torch.from_numpy(imageTransposed)

		#fixationTransposed = np.transpose(sample['fixation'])
		fixationTransposed = np.expand_dims(sample['fixation'], axis=0)
		fixationTransposed = torch.from_numpy(fixationTransposed)

		return {'image': imageTransposed, 'fixation': fixationTransposed, 'image_filename': sample['image_filename']}


class Normalize():
    def __init__(self) -> None:
        pass

    def __call__(self, sample):
        transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        image_transformed = transform(sample['image'])
        return {'image':image_transformed,  'fixation': sample['fixation'], 'image_filename': sample['image_filename']}

def read_file(filename):
	lines = []
	with open(filename, 'r') as file:
	    for line in file: 
	        line = line.strip() #or some other preprocessing
	        lines.append(line)
	return lines

def get_sigmoid(x):
   return 1/(1+np.exp(-x))

# creating the dataset and dataloader
def create_dataloader(base_path, image_filename, fixation_filename, batch_size, shuffle):
    composed = torchvision.transforms.Compose([Rescale(), ToTensor(), Normalize()])
    fixationDS = FixationDataset(base_path, image_filename, fixation_filename, transform=composed)
    # mysample = train_fixationDS[0]
    # print(mysample['image'].shape, mysample['fixation'].shape)
    fixationDL = DataLoader(fixationDS, batch_size=batch_size,shuffle=shuffle)
    return fixationDL

#Training callback to Perform training
def training(base_path, VGG_path, total_epoch, batch_size, learning_rate, save_path):
    #initialize the GPU
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(device)

    #Loading the VGG model
    vgg_conv_weights = torch.load(VGG_path)

    #create the train and validation datatloaders
    train_dl = create_dataloader(base_path, 'train_images.txt', 'train_fixations.txt', batch_size, shuffle=True) #batch_size=16
    valid_dl = create_dataloader(base_path, 'val_images.txt', 'val_fixations.txt', batch_size, shuffle=True) #batch_size=1
    batch = next(iter(train_dl))
    print(f"Image shape: {batch['image'].shape} , fixation shape:{batch['fixation'].shape}")


    #initialize the model object and initiate training & validation
    generator_model = Generator(vgg_conv_weights)
    generator_model.to(device)
    #summary(generator_model, (3, 224, 224))

    #define Loss and SGD optimizer and EPOCH
    loss_fcn = BCELossWithDownsampling()
    params = [par for par in generator_model.parameters() if par.requires_grad]
    opt = torch.optim.Adam(params, lr=learning_rate)
    EPOCHS = total_epoch + 1
    arr_train_loss = []
    arr_valid_loss = []
    arr_train_acc = []
    arr_valid_acc = []

    

    for epoch in range(1, EPOCHS,1):
        generator_model.train()
        train_loss = 0
        correct = 0
        for batch in tqdm(train_dl, desc='Epoch '+str(epoch), total=len(train_dl)):
            image, fixation = batch['image'].to(device), batch['fixation'].to(device) #moving the image and saliency fixation to cuda
            pred = generator_model(image)
            loss = loss_fcn(pred,fixation)
            loss.backward()
            opt.step()
            opt.zero_grad()
            train_loss += loss
            correct += (pred == fixation).float().sum()

        with torch.no_grad():
                # save average loss for evaluation
                train_loss = train_loss / len(train_dl)
                arr_train_loss.append(train_loss.cpu().detach().numpy().tolist())
                train_acc = 100 * correct / len(train_dl)
                arr_train_acc.append(train_acc.cpu().detach().numpy().tolist())

        generator_model.eval()
        with torch.no_grad():
            valid_loss = 0
            correct = 0
            for batch in tqdm(valid_dl, desc='Validation', total=len(valid_dl)):
                val_image, val_fixation = batch['image'].to(device), batch['fixation'].to(device) #moving the image and saliency fixation to cuda
                valid_pred = generator_model(val_image)
                v_loss = loss_fcn(valid_pred, val_fixation)
                valid_loss += v_loss
                correct += (valid_pred == val_fixation).float().sum()
                
            valid_loss = valid_loss/len(valid_dl)
            arr_valid_loss.append(valid_loss.cpu().detach().numpy().tolist())
            val_acc = 100 * correct / len(valid_dl)
            arr_valid_acc.append(val_acc.cpu().detach().numpy().tolist())

            torch.save({
                'epoch': epoch,
                'model_state_dict': generator_model.state_dict(),
                'optimizer_state_dict': opt.state_dict(),
                'train_loss': arr_train_loss,
                'val_loss': arr_valid_loss
            }, os.path.join(save_path, 'epoch-{}.pt'.format(epoch)))

            print(f"Epoch {epoch}, Training loss: {train_loss}, Training acc: {train_acc}, Validation loss: {valid_loss}, Validation acc: {val_acc}")


    #torch.save(generator_model, save_path)
    return arr_train_acc, arr_train_loss, arr_valid_acc, arr_valid_loss

#Plot the loss curves
def show_loss(total_train_loss, total_val_loss, total_train_acc, total_val_acc, fig_save_path):

    # Plot the training and validation accuracy
    plt.figure()
    plt.plot(total_train_acc)
    plt.plot(total_val_acc)
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper right')
    plt.show()
    plt.savefig(fig_save_path+"/Train_Val_Acc.png")

    # Plot the training and validation loss
    plt.figure()
    plt.plot(total_train_loss)
    plt.plot(total_val_loss)
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper right')
    plt.show()
    plt.savefig(fig_save_path+"/Train_Val_Loss.png")

#Make the Saliency predictions
def test_model(base_path, fixation_save_path, model_save_path):
    #create the train and validation datatloaders
    test_dl = create_dataloader(base_path, 'test_images.txt', None, 1, shuffle=False) #batch_size=1
    batch = next(iter(test_dl))
    print(batch)
    print(f"Image shape: {batch['image'].shape}, Image filename: {batch['image_filename']}")

    generator_model = Generator()
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(device)
    generator_model.to(device)

    # Load the last saved model
    saved_model = torch.load(model_save_path, map_location=torch.device(device))
    generator_model.load_state_dict(saved_model['model_state_dict'])
    


    generator_model.eval()
    with torch.no_grad():
        for batch in tqdm(test_dl, desc='Test', total=len(test_dl)):
            test_img = batch['image']
            prediction = generator_model(test_img.to(device))
            print(prediction)
            prediction = prediction.cpu().numpy()
            print(prediction.shape)
            #prediction = get_sigmoid(prediction) 
            prediction = (prediction * 255).astype(np.uint8) #convert the normalized images to RGB image
            prediction = np.squeeze(prediction)
            
            print(prediction)
            print('printing center portion',prediction[100:112,100:112])
            print(prediction.shape)
            prediction = Image.fromarray(prediction)
            
            

            #saving the prediction saliency as image
            head, tail = os.path.split(batch['image_filename'][0])
            image_name = os.path.splitext(tail)[0]
            #imageio.imsave(os.path.join(fixation_save_path+ 'prediction-'+ image_name.split('-')[1]+'.png'),prediction)
            prediction.save(os.path.join(fixation_save_path+ 'prediction-'+ image_name.split('-')[1]+'.png'))


def main():
    
    num_epochs = 40
    batch_size = 16
    learning_rate = 0.001
    is_only_make_predictions = False
    if not is_only_make_predictions:
        #Train the data
        arr_train_acc, arr_train_loss, arr_valid_acc, arr_valid_loss = training(
            base_path= '/export/scratch/CV2',#'/home/anirban/IAS/SEMESTER 4/CV2/Project/Dataset/',
            VGG_path= '/export/scratch/VGG16/vgg16-conv.pth',# '/home/anirban/IAS/SEMESTER 4/CV2/Execises/Ex5/vgg16-conv.pth',
            total_epoch= num_epochs, 
            batch_size= batch_size,
            learning_rate= learning_rate,
            save_path= '/export/scratch/9bhowmic')#'/home/anirban/IAS/SEMESTER 4/CV2/Project') 
        


        print(arr_train_acc,arr_train_loss, arr_valid_acc, arr_valid_loss)
        logs= pd.DataFrame({'epochs':np.arange(1,num_epochs+1,1).tolist(),'train loss':arr_train_loss, 'validation loss':arr_valid_loss})
        logs.to_csv('/export/scratch/9bhowmic/training_logs.csv')


        #Plot the loss
        show_loss(arr_train_loss, arr_valid_loss, 
        arr_train_acc, arr_valid_acc, 
        fig_save_path= '/export/scratch/9bhowmic')#'/home/anirban/IAS/SEMESTER 4/CV2/Project')

        #Make predictions
        test_model(base_path= '/export/scratch/CV2', #'/home/anirban/IAS/SEMESTER 4/CV2/Project/Dataset/',
        fixation_save_path= '/export/scratch/9bhowmic/pred_fixations/', #'/home/anirban/IAS/SEMESTER 4/CV2/Project/Dataset/',
        model_save_path='/export/scratch/9bhowmic/epoch-30.pt') #'/home/anirban/IAS/SEMESTER 4/CV2/Project')

    else:
        test_model(base_path= '/export/scratch/CV2', #'/home/anirban/IAS/SEMESTER 4/CV2/Project/Dataset/',
        fixation_save_path= '/export/scratch/9bhowmic/pred_fixations/', #'/home/anirban/IAS/SEMESTER 4/CV2/Project/Dataset/',
        model_save_path='/export/scratch/9bhowmic/epoch-30.pt') #'/home/anirban/IAS/SEMESTER 4/CV2/Project')



if __name__ == '__main__':
    main()


